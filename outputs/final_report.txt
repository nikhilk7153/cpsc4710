======================================================================
SARM FEATURE CONTROL PPO TRAINING - FINAL REPORT
======================================================================
Date: $(date)

CONFIGURATION
-------------
Policy Model: meta-llama/Llama-3.1-8B-Instruct
Reward Model: Schrieffer/Llama-SARM-4B (4B parameters)
Hardware: 4x NVIDIA H100 GPUs
Training Time: ~6 minutes (100 steps)

PHASE 2: CAUSAL FEATURE DISCOVERY
---------------------------------
Safety-Promoting Features: 23303, 891, 29909, 13159, 18401, 37073, 18583, 40747, 60628, 12021
Safety-Harming Features:   48659, 28879, 26446, 46231, 25241, 53027, 23067, 53670, 12653, 2754

PHASE 3: PPO WITH FEATURE CONTROLS
----------------------------------
Features Penalized: 48659, 28879, 26446 (top 3 safety-harming)
Penalty Threshold (tau): 3.0
Penalty Weight (alpha): 0.1 per feature

Training Progression:
  Step 10:  reward=-0.102, penalty=0.000, loss=0.033
  Step 30:  reward=-0.326, penalty=0.150, loss=0.039
  Step 50:  reward=-0.262, penalty=0.100, loss=0.048
  Step 70:  reward=-0.132, penalty=0.000, loss=0.039
  Step 90:  reward=-0.266, penalty=0.100, loss=0.006
  Step 100: reward=-0.256, penalty=0.100, loss=0.001

RESULTS
-------
The PPO training successfully learned to control feature activations:
- Loss converged from 0.033 to 0.001 (97% reduction)
- Feature penalties oscillated between 0.0 and 0.15, showing the model
  learning to balance reward optimization with feature constraints
- The compound reward (RM score - penalties) remained stable

STEERING EVALUATION (Pre-PPO)
-----------------------------
Dataset                        Before        After      Delta
RB2 Safety Chosen             -0.0228      -0.0228    +0.0000
RB2 Safety Rejected           -0.1065      -0.1057    +0.0008
RMB Safety Chosen             -0.0238      -0.0238    +0.0000
RMB Safety Rejected           -0.1309      -0.1296    +0.0012

Interpretation: Steering shows high specificity - chosen (safe) responses
are unaffected while rejected (unsafe) responses show slight improvement.

OUTPUT FILES
------------
- outputs/ppo_run1/          : PPO-trained policy model (30GB)
- outputs/probes/            : Phase 2 causal probe results
- outputs/eval/              : Steering evaluation results
- outputs/ppo_training.log   : Full training log

======================================================================

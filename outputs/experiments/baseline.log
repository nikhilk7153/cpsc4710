W1212 23:20:13.041000 95270 site-packages/torch/distributed/run.py:803] 
W1212 23:20:13.041000 95270 site-packages/torch/distributed/run.py:803] *****************************************
W1212 23:20:13.041000 95270 site-packages/torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1212 23:20:13.041000 95270 site-packages/torch/distributed/run.py:803] *****************************************
/home/ubuntu/miniconda3/envs/sarm/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/ubuntu/miniconda3/envs/sarm/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/ubuntu/miniconda3/envs/sarm/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/ubuntu/miniconda3/envs/sarm/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(

============================================================
PPO Training with SARM + LoRA
============================================================
GPUs: 4
Policy: meta-llama/Llama-3.1-8B-Instruct
SARM: Schrieffer/Llama-SARM-4B
LoRA: r=16, alpha=32
============================================================

You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading policy model with LoRA...
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  6.44it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  6.55it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  6.21it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  5.93it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  6.48it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  6.67it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  6.29it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  5.99it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  6.71it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  6.51it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  6.30it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  6.83it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  6.76it/s]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  6.00it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  6.62it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  6.57it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  6.58it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  6.46it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  6.50it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  6.30it/s]
trainable params: 13,631,488 || all params: 8,043,892,736 || trainable%: 0.1695
Loading SARM reward model...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.05it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.06s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.20s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.05it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.04it/s]

Feature Controls:

Before initializing optimizer states
MA 23.52 GB         Max_MA 23.54 GB         CA 23.76 GB         Max_CA 24 GB 
CPU Virtual Memory:  used = 19.99 GB, percent = 2.3%
After initializing optimizer states
MA 23.52 GB         Max_MA 23.55 GB         CA 23.79 GB         Max_CA 24 GB 
CPU Virtual Memory:  used = 19.99 GB, percent = 2.3%
After initializing ZeRO optimizer
MA 23.52 GB         Max_MA 23.52 GB         CA 23.79 GB         Max_CA 24 GB 
CPU Virtual Memory:  used = 19.99 GB, percent = 2.3%
Training:   0%|          | 0/50 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)
We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)
We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)
We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)
Training:   2%|▏         | 1/50 [00:04<04:02,  4.95s/it]Training:   4%|▍         | 2/50 [00:09<03:32,  4.43s/it]Training:   6%|▌         | 3/50 [00:12<03:15,  4.16s/it]Training:   8%|▊         | 4/50 [00:16<03:07,  4.07s/it]Training:  10%|█         | 5/50 [00:20<02:57,  3.94s/it]Training:  12%|█▏        | 6/50 [00:24<02:48,  3.82s/it]Training:  14%|█▍        | 7/50 [00:28<02:52,  4.02s/it]Training:  16%|█▌        | 8/50 [00:32<02:46,  3.97s/it]Training:  18%|█▊        | 9/50 [00:36<02:39,  3.90s/it]Training:  20%|██        | 10/50 [00:39<02:34,  3.87s/it]Training:  20%|██        | 10/50 [00:39<02:34,  3.87s/it, reward=-0.148, rm=-0.148, pen=0.000, loss=0.489]Training:  22%|██▏       | 11/50 [00:43<02:26,  3.75s/it, reward=-0.148, rm=-0.148, pen=0.000, loss=0.489]Training:  24%|██▍       | 12/50 [00:46<02:19,  3.67s/it, reward=-0.148, rm=-0.148, pen=0.000, loss=0.489]Training:  26%|██▌       | 13/50 [00:50<02:13,  3.62s/it, reward=-0.148, rm=-0.148, pen=0.000, loss=0.489]Training:  28%|██▊       | 14/50 [00:54<02:12,  3.68s/it, reward=-0.148, rm=-0.148, pen=0.000, loss=0.489]Training:  30%|███       | 15/50 [00:57<02:06,  3.61s/it, reward=-0.148, rm=-0.148, pen=0.000, loss=0.489]
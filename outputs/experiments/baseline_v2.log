W1213 02:16:36.421000 133970 site-packages/torch/distributed/run.py:803] 
W1213 02:16:36.421000 133970 site-packages/torch/distributed/run.py:803] *****************************************
W1213 02:16:36.421000 133970 site-packages/torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1213 02:16:36.421000 133970 site-packages/torch/distributed/run.py:803] *****************************************
/home/ubuntu/miniconda3/envs/sarm/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/ubuntu/miniconda3/envs/sarm/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/ubuntu/miniconda3/envs/sarm/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/ubuntu/miniconda3/envs/sarm/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(

============================================================
PPO Training with SARM + LoRA
============================================================
GPUs: 4
Policy: meta-llama/Llama-3.1-8B-Instruct
SARM: Schrieffer/Llama-SARM-4B
LoRA: r=16, alpha=32
============================================================

Loading policy model with LoRA...
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  6.18it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  6.20it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  6.06it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  6.52it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  6.27it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  6.29it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  6.61it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  6.18it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  6.31it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  6.32it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  6.64it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  6.22it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  6.39it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  6.34it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  6.40it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  6.35it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  6.71it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  6.67it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  6.33it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  6.27it/s]
trainable params: 13,631,488 || all params: 8,043,892,736 || trainable%: 0.1695
Loading SARM reward model...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.04it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.02it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.07it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.11it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.11it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.03it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.00it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.00it/s]

Feature Controls:

Before initializing optimizer states
MA 23.52 GB         Max_MA 23.54 GB         CA 23.76 GB         Max_CA 24 GB 
CPU Virtual Memory:  used = 19.83 GB, percent = 2.2%
After initializing optimizer states
MA 23.52 GB         Max_MA 23.55 GB         CA 23.79 GB         Max_CA 24 GB 
CPU Virtual Memory:  used = 19.83 GB, percent = 2.2%
After initializing ZeRO optimizer
MA 23.52 GB         Max_MA 23.52 GB         CA 23.79 GB         Max_CA 24 GB 
CPU Virtual Memory:  used = 19.83 GB, percent = 2.2%
Training:   0%|          | 0/100 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)
We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)
We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)
We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)
Training:   1%|          | 1/100 [00:08<14:20,  8.69s/it]Training:   2%|▏         | 2/100 [00:16<13:05,  8.01s/it]Training:   3%|▎         | 3/100 [00:23<12:12,  7.55s/it]Training:   4%|▍         | 4/100 [00:30<12:06,  7.57s/it]Training:   5%|▌         | 5/100 [00:37<11:34,  7.31s/it]Training:   6%|▌         | 6/100 [00:44<11:04,  7.07s/it]Training:   7%|▋         | 7/100 [00:52<11:43,  7.56s/it]Training:   8%|▊         | 8/100 [00:59<11:15,  7.34s/it]Training:   9%|▉         | 9/100 [01:06<10:50,  7.15s/it]Training:  10%|█         | 10/100 [01:13<10:35,  7.06s/it]Training:  10%|█         | 10/100 [01:13<10:35,  7.06s/it, reward=-0.093, rm=-0.093, pen=0.000, loss=2.968]Training:  11%|█         | 11/100 [01:19<10:17,  6.94s/it, reward=-0.093, rm=-0.093, pen=0.000, loss=2.968]Training:  12%|█▏        | 12/100 [01:26<10:01,  6.84s/it, reward=-0.093, rm=-0.093, pen=0.000, loss=2.968]Training:  13%|█▎        | 13/100 [01:33<09:48,  6.76s/it, reward=-0.093, rm=-0.093, pen=0.000, loss=2.968]Training:  14%|█▍        | 14/100 [01:40<09:45,  6.80s/it, reward=-0.093, rm=-0.093, pen=0.000, loss=2.968]Training:  15%|█▌        | 15/100 [01:46<09:34,  6.76s/it, reward=-0.093, rm=-0.093, pen=0.000, loss=2.968]Training:  16%|█▌        | 16/100 [01:53<09:25,  6.74s/it, reward=-0.093, rm=-0.093, pen=0.000, loss=2.968]Training:  17%|█▋        | 17/100 [02:00<09:17,  6.72s/it, reward=-0.093, rm=-0.093, pen=0.000, loss=2.968]Training:  18%|█▊        | 18/100 [02:06<09:10,  6.71s/it, reward=-0.093, rm=-0.093, pen=0.000, loss=2.968]Training:  19%|█▉        | 19/100 [02:13<09:01,  6.69s/it, reward=-0.093, rm=-0.093, pen=0.000, loss=2.968]Training:  20%|██        | 20/100 [02:20<08:55,  6.69s/it, reward=-0.093, rm=-0.093, pen=0.000, loss=2.968]Training:  20%|██        | 20/100 [02:20<08:55,  6.69s/it, reward=-0.086, rm=-0.086, pen=0.000, loss=5.128]Training:  21%|██        | 21/100 [02:27<09:00,  6.84s/it, reward=-0.086, rm=-0.086, pen=0.000, loss=5.128]Training:  22%|██▏       | 22/100 [02:34<08:55,  6.86s/it, reward=-0.086, rm=-0.086, pen=0.000, loss=5.128]Training:  23%|██▎       | 23/100 [02:40<08:43,  6.79s/it, reward=-0.086, rm=-0.086, pen=0.000, loss=5.128]Training:  24%|██▍       | 24/100 [02:47<08:42,  6.87s/it, reward=-0.086, rm=-0.086, pen=0.000, loss=5.128]Training:  25%|██▌       | 25/100 [02:54<08:29,  6.79s/it, reward=-0.086, rm=-0.086, pen=0.000, loss=5.128]Training:  26%|██▌       | 26/100 [03:01<08:19,  6.75s/it, reward=-0.086, rm=-0.086, pen=0.000, loss=5.128]Training:  27%|██▋       | 27/100 [03:07<08:09,  6.71s/it, reward=-0.086, rm=-0.086, pen=0.000, loss=5.128]Training:  28%|██▊       | 28/100 [03:14<08:01,  6.69s/it, reward=-0.086, rm=-0.086, pen=0.000, loss=5.128]Training:  29%|██▉       | 29/100 [03:21<07:54,  6.68s/it, reward=-0.086, rm=-0.086, pen=0.000, loss=5.128]Training:  30%|███       | 30/100 [03:27<07:46,  6.66s/it, reward=-0.086, rm=-0.086, pen=0.000, loss=5.128]Training:  30%|███       | 30/100 [03:27<07:46,  6.66s/it, reward=-0.111, rm=-0.111, pen=0.000, loss=0.243]Training:  31%|███       | 31/100 [03:34<07:34,  6.59s/it, reward=-0.111, rm=-0.111, pen=0.000, loss=0.243]Training:  32%|███▏      | 32/100 [03:40<07:29,  6.61s/it, reward=-0.111, rm=-0.111, pen=0.000, loss=0.243]Training:  33%|███▎      | 33/100 [03:47<07:26,  6.67s/it, reward=-0.111, rm=-0.111, pen=0.000, loss=0.243]Training:  34%|███▍      | 34/100 [03:54<07:19,  6.65s/it, reward=-0.111, rm=-0.111, pen=0.000, loss=0.243]Training:  35%|███▌      | 35/100 [04:00<07:12,  6.65s/it, reward=-0.111, rm=-0.111, pen=0.000, loss=0.243]Training:  36%|███▌      | 36/100 [04:07<07:04,  6.64s/it, reward=-0.111, rm=-0.111, pen=0.000, loss=0.243]Training:  37%|███▋      | 37/100 [04:14<06:58,  6.65s/it, reward=-0.111, rm=-0.111, pen=0.000, loss=0.243]Training:  38%|███▊      | 38/100 [04:20<06:52,  6.65s/it, reward=-0.111, rm=-0.111, pen=0.000, loss=0.243]Training:  39%|███▉      | 39/100 [04:27<06:45,  6.66s/it, reward=-0.111, rm=-0.111, pen=0.000, loss=0.243]Training:  40%|████      | 40/100 [04:34<06:40,  6.67s/it, reward=-0.111, rm=-0.111, pen=0.000, loss=0.243]Training:  40%|████      | 40/100 [04:34<06:40,  6.67s/it, reward=-0.152, rm=-0.152, pen=0.000, loss=0.363]Training:  41%|████      | 41/100 [04:41<06:51,  6.97s/it, reward=-0.152, rm=-0.152, pen=0.000, loss=0.363]Training:  42%|████▏     | 42/100 [04:48<06:39,  6.88s/it, reward=-0.152, rm=-0.152, pen=0.000, loss=0.363]Training:  43%|████▎     | 43/100 [04:55<06:28,  6.82s/it, reward=-0.152, rm=-0.152, pen=0.000, loss=0.363]Training:  44%|████▍     | 44/100 [05:01<06:19,  6.77s/it, reward=-0.152, rm=-0.152, pen=0.000, loss=0.363]Training:  45%|████▌     | 45/100 [05:08<06:10,  6.74s/it, reward=-0.152, rm=-0.152, pen=0.000, loss=0.363]Training:  46%|████▌     | 46/100 [05:15<06:02,  6.71s/it, reward=-0.152, rm=-0.152, pen=0.000, loss=0.363]Training:  47%|████▋     | 47/100 [05:21<05:54,  6.69s/it, reward=-0.152, rm=-0.152, pen=0.000, loss=0.363]Training:  48%|████▊     | 48/100 [05:28<05:49,  6.72s/it, reward=-0.152, rm=-0.152, pen=0.000, loss=0.363]